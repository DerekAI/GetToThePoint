{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 0. import packages\n",
    "import spacy\n",
    "import os\n",
    "from collections import Counter\n",
    "import torch\n",
    "import glob\n",
    "from functions import parse_cnn, word_list_to_idx_list\n",
    "from spacy import attrs\n",
    "import numpy as np\n",
    "vocab_size = 50000\n",
    "batch_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1. load nlp model & files to read\n",
    "nlp = spacy.load('en') # loads default English object\n",
    "cnn_dir = '/home/mjc/datasets/CNN_DailyMail/cnn/stories/'\n",
    "cnn_pre_dir = '/home/mjc/datasets/CNN_DailyMail/cnn/stories_idx/'\n",
    "file_list = [os.path.join(cnn_dir,file) for file in os.listdir(cnn_dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary created from 1000/92579 files, top 33394 words saved\n",
      "Vocabulary created from 2000/92579 files, top 46143 words saved\n",
      "Vocabulary created from 3000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 4000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 5000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 6000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 7000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 8000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 9000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 10000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 11000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 12000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 13000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 14000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 15000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 16000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 17000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 18000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 19000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 20000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 21000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 22000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 23000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 24000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 25000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 26000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 27000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 28000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 29000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 30000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 31000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 32000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 33000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 34000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 35000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 36000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 37000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 38000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 39000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 40000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 41000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 42000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 43000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 44000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 45000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 46000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 47000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 48000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 49000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 50000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 51000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 52000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 53000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 54000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 55000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 56000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 57000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 58000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 59000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 60000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 61000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 62000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 63000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 64000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 65000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 66000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 67000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 68000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 69000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 70000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 71000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 72000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 73000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 74000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 75000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 76000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 77000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 78000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 79000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 80000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 81000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 82000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 83000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 84000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 85000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 86000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 87000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 88000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 89000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 90000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 91000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 92000/92579 files, top 50001 words saved\n",
      "Vocabulary created from 93000/92579 files, top 50001 words saved\n"
     ]
    }
   ],
   "source": [
    "# 2. create dictionary using frequent words\n",
    "# body_list = []\n",
    "# summary_list = []\n",
    "# counter = Counter()\n",
    "# batch_no = 0\n",
    "# while batch_no<len(file_list):\n",
    "#     batch = file_list[batch_no:min(batch_no+batch_size,len(file_list))]\n",
    "#     for file in batch:\n",
    "#         body_words, summary_words = parse_cnn(file,nlp)\n",
    "#         body_list.extend(body_words)\n",
    "#         summary_list.extend(summary_words)\n",
    "#     c = Counter(body_list+summary_list)\n",
    "#     counter = counter + c\n",
    "#     vocab_list = counter.most_common(vocab_size)\n",
    "#     word2idx = dict()\n",
    "#     word2idx['<PAD>']=0\n",
    "#     word2idx['<S>']=1\n",
    "#     word2idx['</S>']=2\n",
    "#     word2idx['<UNK>']=3\n",
    "#     idx2word = dict()\n",
    "#     idx2word[0] = '<PAD>'\n",
    "#     idx2word[1] = '<S>'\n",
    "#     idx2word[2] = '</S>'\n",
    "#     idx2word[3] = '<UNK>'\n",
    "#     for i,(word,_) in enumerate(vocab_list):\n",
    "#         if len(word2idx)>vocab_size:\n",
    "#             break\n",
    "#         word2idx[word] = i+4\n",
    "#         idx2word[i+4] = word\n",
    "#     np.save('word2idx.npy',word2idx)\n",
    "#     np.save('idx2word.npy',idx2word)\n",
    "#     batch_no+=batch_size\n",
    "#     print(\"Vocabulary created from %d/%d files, top %d words saved\" \n",
    "#           %(batch_no,len(file_list),len(word2idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 files processed so far\n",
      "2000 files processed so far\n",
      "3000 files processed so far\n",
      "4000 files processed so far\n",
      "5000 files processed so far\n",
      "6000 files processed so far\n",
      "7000 files processed so far\n",
      "8000 files processed so far\n",
      "9000 files processed so far\n",
      "10000 files processed so far\n",
      "11000 files processed so far\n",
      "12000 files processed so far\n",
      "13000 files processed so far\n",
      "14000 files processed so far\n",
      "15000 files processed so far\n",
      "16000 files processed so far\n",
      "17000 files processed so far\n",
      "18000 files processed so far\n",
      "19000 files processed so far\n",
      "20000 files processed so far\n",
      "21000 files processed so far\n",
      "22000 files processed so far\n",
      "23000 files processed so far\n",
      "24000 files processed so far\n",
      "25000 files processed so far\n",
      "26000 files processed so far\n",
      "27000 files processed so far\n",
      "28000 files processed so far\n",
      "29000 files processed so far\n",
      "30000 files processed so far\n",
      "31000 files processed so far\n",
      "32000 files processed so far\n",
      "33000 files processed so far\n",
      "34000 files processed so far\n",
      "35000 files processed so far\n",
      "36000 files processed so far\n",
      "37000 files processed so far\n",
      "38000 files processed so far\n",
      "39000 files processed so far\n",
      "40000 files processed so far\n",
      "41000 files processed so far\n",
      "42000 files processed so far\n",
      "43000 files processed so far\n",
      "44000 files processed so far\n",
      "45000 files processed so far\n",
      "46000 files processed so far\n",
      "47000 files processed so far\n",
      "48000 files processed so far\n",
      "49000 files processed so far\n",
      "50000 files processed so far\n",
      "51000 files processed so far\n",
      "52000 files processed so far\n",
      "53000 files processed so far\n",
      "54000 files processed so far\n",
      "55000 files processed so far\n",
      "56000 files processed so far\n",
      "57000 files processed so far\n",
      "58000 files processed so far\n",
      "59000 files processed so far\n",
      "60000 files processed so far\n",
      "61000 files processed so far\n",
      "62000 files processed so far\n",
      "63000 files processed so far\n",
      "64000 files processed so far\n",
      "65000 files processed so far\n",
      "66000 files processed so far\n",
      "67000 files processed so far\n",
      "68000 files processed so far\n",
      "69000 files processed so far\n",
      "70000 files processed so far\n",
      "71000 files processed so far\n",
      "72000 files processed so far\n",
      "73000 files processed so far\n",
      "74000 files processed so far\n",
      "75000 files processed so far\n",
      "76000 files processed so far\n",
      "77000 files processed so far\n",
      "78000 files processed so far\n",
      "79000 files processed so far\n",
      "80000 files processed so far\n",
      "81000 files processed so far\n",
      "82000 files processed so far\n",
      "83000 files processed so far\n",
      "84000 files processed so far\n",
      "85000 files processed so far\n",
      "86000 files processed so far\n",
      "87000 files processed so far\n",
      "88000 files processed so far\n",
      "89000 files processed so far\n",
      "90000 files processed so far\n",
      "91000 files processed so far\n",
      "92000 files processed so far\n"
     ]
    }
   ],
   "source": [
    "w2i = np.load('word2idx.npy').item()\n",
    "i2w = np.load('idx2word.npy').item()\n",
    "v = len(w2i)\n",
    "# 3. preprocess each document in CNN so that we get a form where a text is seen in vectors\n",
    "out_file_list = [os.path.join(cnn_pre_dir,file) for file in os.listdir(cnn_dir)]\n",
    "in_out_zip = zip(file_list, out_file_list)\n",
    "cnt = 0\n",
    "for in_file, out_file in in_out_zip:\n",
    "    body_words, summary_words = parse_cnn(in_file, nlp)\n",
    "    body_idx = word_list_to_idx_list(body_words, w2i, v)\n",
    "    body_idx = [str(x) for x in body_idx]\n",
    "    summary_idx = word_list_to_idx_list(summary_words,w2i,v)\n",
    "    summary_idx = [str(x) for x in summary_idx]\n",
    "    out = ' '.join(body_idx)+\"::\"+' '.join(summary_idx)\n",
    "    with open(out_file,'w') as f:\n",
    "        f.write(out)\n",
    "    cnt+=1\n",
    "    if cnt%1000==0:\n",
    "        print('%d files processed so far' %(cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " 'a',\n",
       " 'south',\n",
       " 'korean',\n",
       " 'official',\n",
       " 'says',\n",
       " 'jill',\n",
       " 'kelley',\n",
       " \"'s\",\n",
       " 'use',\n",
       " 'of',\n",
       " 'her',\n",
       " 'honorary',\n",
       " 'title',\n",
       " 'was',\n",
       " '\"',\n",
       " 'not',\n",
       " 'suitable',\n",
       " '\"',\n",
       " '.',\n",
       " ' ',\n",
       " 'a',\n",
       " 'new',\n",
       " 'york',\n",
       " 'businessman',\n",
       " 'accused',\n",
       " 'her',\n",
       " 'of',\n",
       " 'using',\n",
       " 'that',\n",
       " 'designation',\n",
       " 'to',\n",
       " 'solicit',\n",
       " 'business',\n",
       " '.',\n",
       " ' ',\n",
       " 'kelley',\n",
       " \"'s\",\n",
       " 'complaint',\n",
       " 'about',\n",
       " 'harassing',\n",
       " 'e',\n",
       " '-',\n",
       " 'mails',\n",
       " 'led',\n",
       " 'to',\n",
       " 'the',\n",
       " 'resignation',\n",
       " 'of',\n",
       " 'cia',\n",
       " 'chief',\n",
       " 'david',\n",
       " 'petraeus',\n",
       " '.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " 'tv',\n",
       " 'personality',\n",
       " 'star',\n",
       " 'jones',\n",
       " 'was',\n",
       " 'diagnosed',\n",
       " 'with',\n",
       " 'heart',\n",
       " 'disease',\n",
       " 'in',\n",
       " '2010',\n",
       " '.',\n",
       " ' ',\n",
       " 'heart',\n",
       " 'disease',\n",
       " 'is',\n",
       " 'the',\n",
       " 'leading',\n",
       " 'cause',\n",
       " 'of',\n",
       " 'death',\n",
       " 'for',\n",
       " 'men',\n",
       " 'and',\n",
       " 'women',\n",
       " ',',\n",
       " 'but',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'preventable',\n",
       " '.',\n",
       " ' ',\n",
       " 'february',\n",
       " 'is',\n",
       " 'american',\n",
       " 'heart',\n",
       " 'month',\n",
       " ',',\n",
       " 'and',\n",
       " 'friday',\n",
       " 'is',\n",
       " 'national',\n",
       " 'wear',\n",
       " 'red',\n",
       " 'day',\n",
       " '.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp = []\n",
    "for i,j in a:\n",
    "    tmp.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(file_list[0]) as f:\n",
    "    text = f.read()\n",
    "    text = text.lower()\n",
    "    text = text.replace('\\n\\n',' ')\n",
    "    text = text.replace('(cnn)','')\n",
    "    text = text.split(\"@highlight\")\n",
    "    body = text[0]\n",
    "    body_tokens = nlp(body)\n",
    "    summaries = text[1:]\n",
    "    summary_tokens = nlp(' '.join([x.strip()+'.' for x in summaries])+'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w2i = dict()\n",
    "w2i['<PAD>']=0\n",
    "w2i['<S>']=1\n",
    "w2i['</S>']=2\n",
    "\n",
    "i2w = dict()\n",
    "i2w[0]='<PAD>'\n",
    "i2w[1]='<S>'\n",
    "i2w[2]='</S>'\n",
    "\n",
    "for i,word in enumerate(word2idx):\n",
    "    if len(w2i)>50000:\n",
    "        break\n",
    "    w2i[word] = i+3\n",
    "    i2w[i+3] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i2w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def nlp_to_tokens(token_list,word2idx):\n",
    "    out = []\n",
    "    oov2idx = dict()\n",
    "    oov_idx = 0\n",
    "    for token in token_list:\n",
    "        word = token.text\n",
    "        try:\n",
    "            out.append(word2idx[word])\n",
    "        except KeyError:\n",
    "            if word not in oov2idx:\n",
    "                oov_idx+=1\n",
    "                oov2idx[word]=vocab_size+oov_idx\n",
    "            out.append(oov2idx[word])\n",
    "    return out, oov2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out, oov2idx = nlp_to_tokens(list(body_tokens),word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word2idx[l[1].text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc = nlp(body)\n",
    "lst = list(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = list(set(lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out= []\n",
    "oov_dict = dict()\n",
    "for x in words:\n",
    "    try:\n",
    "        out.append(word2idx[x])\n",
    "    except KeyError:\n",
    "        oov_dict[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word2idx['oifdjherht']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c.most_common(300)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = list(np.arange(32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i=0\n",
    "while (i<10):\n",
    "    i+=1\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_list = []\n",
    "i = 0\n",
    "for file_name in file_list:\n",
    "    with open(file_name) as f:\n",
    "        text = f.read()\n",
    "        text = text.lower()\n",
    "        text = text.replace('\\n\\n',' ')\n",
    "        text = text.replace('(cnn)','')\n",
    "        text = text.split(\"@highlight\")\n",
    "        body = text[0]\n",
    "        doc = list(nlp(body))\n",
    "        word_list.extend([x.text for x in doc])\n",
    "    if i%1000==0:\n",
    "        print(i)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = c + Counter(['a','b','a','b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(list(set(word_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "a = Variable(torch.LongTensor(np.arange(40).reshape(4,10)))\n",
    "emb = nn.Embedding(40,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "help(nn.LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(hidden_size=100,input_size=20, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A=np.arange(24).reshape(4,6)\n",
    "A=A*(-1)\n",
    "A=A+15\n",
    "A = np.maximum(A,0)\n",
    "A = Variable(torch.LongTensor(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "B = A==0\n",
    "B.float().data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = Variable(torch.Tensor(1,4,100))\n",
    "out=lstm(emb(a[:,0].unsqueeze(1)), (c,c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emb(a).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = Counter(['a','a','a','a','a','b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "import os\n",
    "from collections import Counter\n",
    "import torch\n",
    "import glob\n",
    "from spacy import attrs\n",
    "\n",
    "\n",
    "word2idx = np.load('word2idx.npy').item()\n",
    "vocab_size = len(word2idx)\n",
    "batch_size = 1000\n",
    "\n",
    "nlp = spacy.load('en') # loads default English object\n",
    "cnn_dir = '/home/mjc/datasets/CNN_DailyMail/cnn/stories/'\n",
    "cnn_pre_dir = '/home/mjc/datasets/CNN_DailyMail/cnn/preprocessed_stories/'\n",
    "\n",
    "file_list = [os.path.join(cnn_dir,file) for file in os.listdir(cnn_dir)]\n",
    "total_files = len(file_list)\n",
    "files_read = 0\n",
    "count = 0\n",
    "for file in file_list[0:1]:\n",
    "    with open(file) as f:\n",
    "        text = f.read()\n",
    "        print(text)\n",
    "        text = text.lower()\n",
    "        text = text.replace('\\n\\n',' ')\n",
    "        text = text.replace('(cnn)','')\n",
    "        text = text.split(\"@highlight\")\n",
    "        body = text[0]\n",
    "        body_words = body.split(' ')\n",
    "        summaries = ' . '.join(text[1:])+' .'\n",
    "        summary_words = summaries.split(' ')\n",
    "        unique_words = list(set(body_words+summary_words))\n",
    "        temp_dict = dict()\n",
    "        oovs = 0\n",
    "        for w in unique_words:\n",
    "            try:\n",
    "                temp_dict[w] = word2idx[w]\n",
    "            except KeyError:\n",
    "                oovs+=1\n",
    "                temp_dict[w] = oovs+vocab_size\n",
    "        body_idx = [str(temp_dict[x]) for x in body_words]\n",
    "        summary_idx = [str(temp_dict[x]) for x in summary_words]\n",
    "        out = ' '.join(body_idx)+'::'+' '.join(summary_idx)\n",
    "        out_file = file.replace('/stories/','/preprocessed_stories/')\n",
    "    with open(out_file,'w') as f:\n",
    "        f.write(out)\n",
    "    count+=1\n",
    "    if count%100==0:\n",
    "        print(count)\n",
    "\n",
    "\n",
    "# \t\tdoc = nlp(text)\n",
    "\n",
    "\n",
    "# counter = Counter()\n",
    "# while (files_read<total_files):\n",
    "#     word_list = []\n",
    "#     batch_files = file_list[files_read:min(files_read+1000,total_files)]\n",
    "#     for file_name in batch_files:\n",
    "#         with open(file_name) as f:\n",
    "#             text = f.read()\n",
    "#             text = text.lower()\n",
    "#             text = text.replace('\\n\\n',' ')\n",
    "#             text = text.replace('(cnn)','')\n",
    "#             text = text.split(\"@highlight\")\n",
    "#             body = text[0]\n",
    "#             doc = list(nlp(body))\n",
    "#             word_list.extend([x.text for x in doc])\n",
    "\n",
    "#     counter = counter + Counter(word_list)\n",
    "#     files_read+=len(batch_files)\n",
    "#     print(\"%d files read so far...\" % files_read)\n",
    "#     word2idx = {tup[0]: i for i,tup in enumerate(counter.most_common(vocab_size))}\n",
    "#     np.save('word2idx.npy',word2idx)\n",
    "# print(\"All merged!\")\n",
    "# word2idx = {tup[0]: i for i,tup in enumerate(counter.most_common(vocab_size))}\n",
    "# np.save('word2idx.npy',word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
