{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# arguments related to the dataset\n",
    "parser.add_argument(\"--data_dir\",type=str, default='/home/mjc/datasets/CNN_DailyMail/cnn/stories_merged_100/',\n",
    "                    help='directory where data files are located')\n",
    "parser.add_argument(\"--word2idx\",type=str, default='word2idx.npy', help='file name for word2idx file')\n",
    "parser.add_argument(\"--idx2word\",type=str, default='idx2word.npy', help='file name for idx2word file')\n",
    "parser.add_argument(\"--max_enc\",type=int, default=400, help='max length of encoder sequence')\n",
    "parser.add_argument(\"--max_dec\",type=int, default=100, help='max length of decoder sequence')\n",
    "parser.add_argument(\"--min_dec\",type=int, default=35, help='min length of decoder sequence')\n",
    "parser.add_argument(\"--vocab_size\",type=int, default=50000, help='vocabulary size')\n",
    "parser.add_argument(\"--max_oovs\",type=int, default=20, help='max number of OOVs to accept in a sample')\n",
    "\n",
    "\n",
    "# arguments related to model training and inference\n",
    "parser.add_argument(\"--train\",type=bool, default=True, help='train/test model. Set by default to True(=train)')\n",
    "parser.add_argument(\"--epochs\",type=int, default=20, help='Number of epochs. Set by default to 20')\n",
    "parser.add_argument(\"--load_model\",type=str, default='', help='input model name to start from a pretrained model')\n",
    "parser.add_argument(\"--hidden\",type=int, default=256, help='size of hidden dimension')\n",
    "parser.add_argument(\"--embed\",type=int, default=128, help='size of embedded word dimension')\n",
    "parser.add_argument(\"--lr\",type=float, default=0.15, help='learning rate')\n",
    "parser.add_argument(\"--cov_lambda\",type=float, default=1.0, help='lambda for coverage loss')\n",
    "parser.add_argument(\"--beam\",type=int, default=4, help='beam size')\n",
    "parser.add_argument(\"--cuda\",type=bool, default=True, help='whether to use GPU')\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "from packages.vocab import Vocab\n",
    "from packages.batch import Batch\n",
    "from model import Model\n",
    "from packages.functions import to_cuda, num_to_var\n",
    "import torch\n",
    "from torch import nn,optim\n",
    "import os\n",
    "import random\n",
    "from torch.nn.utils.rnn import pack_padded_sequence as pack\n",
    "from torch.nn.utils.rnn import pad_packed_sequence as pad\n",
    "\n",
    "parser = argparse.ArgumentParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Args(object):\n",
    "    data_dir='/home/irteam/users/data/CNN_DailyMail/cnn/2.stories_tokenized_100/'\n",
    "    word2idx='word2idx.npy'\n",
    "    idx2word='idx2word.npy'\n",
    "    max_enc=200\n",
    "    max_dec=50\n",
    "    min_dec=35\n",
    "    vocab_size=50000\n",
    "    max_oovs = 20\n",
    "    \n",
    "    train = True\n",
    "    epochs = 20\n",
    "    load_model = ''\n",
    "    hidden_size = 256\n",
    "    embed_size = 128\n",
    "    lr = 0.15\n",
    "    cov_lambda = 1.0\n",
    "    beam = 4\n",
    "    cuda = True\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def main(args):\n",
    "# obtain vocabulary\n",
    "vocab = Vocab(args.vocab_size)\n",
    "vocab.w2i = np.load(args.word2idx).item()\n",
    "vocab.i2w = np.load(args.idx2word).item()\n",
    "vocab.count = len(vocab.w2i)\n",
    "\n",
    "# obtain dataset in batches\n",
    "file_list = os.listdir(args.data_dir)\n",
    "batch = Batch(file_list, args.max_enc, args.max_dec, args.max_oovs)\n",
    "\n",
    "# load model\n",
    "if args.load_model != '':\n",
    "    model = torch.load(args.load_model)\n",
    "else:\n",
    "    model = Model(args)\n",
    "model = to_cuda(model)\n",
    "\n",
    "# get loss and optimizers\n",
    "opt = optim.Adam(params=model.parameters(),lr=args.lr)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# computation for each epoch\n",
    "epoch = 0\n",
    "while (epoch<args.epochs):\n",
    "    epoch+=1\n",
    "    random.shuffle(file_list)\n",
    "    for file in file_list:\n",
    "        opt.zero_grad()\n",
    "        with open(os.path.join(args.data_dir,file)) as f:\n",
    "            minibatch = f.read()\n",
    "        stories,summaries = batch.process_minibatch(minibatch,vocab)\n",
    "        out_list, cov_loss = model(stories, summaries, batch, vocab, True)\n",
    "        \n",
    "        # get packed versions\n",
    "        target = num_to_var(summaries[:,1:])\n",
    "        target = pack(target, batch.output_lens.to_list(),batch_first=True)[0]\n",
    "        pad_out = pack(out_list, batch.output_lens.to_list(),batch_first=True)[0]\n",
    "        pad_out = torch.log(pad_out)\n",
    "        loss = criterion(pad_out,target)+cov_loss\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        print(\"got thru batch!\")\n",
    "# if __name__ == \"__main__\":\n",
    "#     main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.zeros([10,40],dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for selective masking while preserving sizes\n",
    "from collections import Counter \n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "numbers = stories.reshape(-1).tolist()\n",
    "set_numbers = list(set(numbers))\n",
    "c = Counter(numbers)\n",
    "dup_list = [k for k in set_numbers if c[k] > 1]\n",
    "print(\"Number of iters: %d\" % len(dup_list))\n",
    "elapsed = time.time()\n",
    "print(\"Time elapsed: \", elapsed-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dup in dup_list[0:1]:\n",
    "    mask = np.array(stories==dup,dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = np.random.randn(mask.shape[0],mask.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = mask*attn.sum(1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "mask2 = Variable(torch.Tensor(mask).cuda())\n",
    "attn2 = Variable(torch.Tensor(attn).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M2=torch.mul(mask2, attn2.sum(1).unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn2[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "hidden_dim = 256\n",
    "embed_dim = 128\n",
    "batch_size = 16\n",
    "num_samples = 92579\n",
    "max_encoder_steps = 400\n",
    "max_decoder_steps = 100\n",
    "beam_size = 4\n",
    "min_decoder_steps = 35 # min size of generated sequence\n",
    "vocab_size = 50000\n",
    "lr = 0.15\n",
    "adagrad_init_acc = 0.1 # deprecated for pytorch\n",
    "rand_unif_init_mag = 0.02 # magnitude for lstm cells during random init\n",
    "trunc_norm_init_std = 1e-4 # std of truncated norm initialization\n",
    "max_grad_norm = 2.0 # so they do apply gradient clipping\n",
    "max_oovs = 20 # maximum number of oovs allowed?\n",
    "coverage_loss = 1.0 # lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get vocabulary\n",
    "vocab = Vocab(50000)\n",
    "vocab.w2i = np.load('word2idx.npy').item()\n",
    "vocab.i2w = np.load('idx2word.npy').item()\n",
    "vocab.count = len(vocab.w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get dataset in batches\n",
    "file_dir = '/home/mjc/datasets/CNN_DailyMail/cnn/stories_merged_100/'\n",
    "file_list = os.listdir(file_dir)\n",
    "batch = Batch(file_list,400,100,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch.init_minibatch()\n",
    "with open(os.path.join(file_dir,file_list[70])) as f:\n",
    "    minibatch = f.read()\n",
    "    minibatch = minibatch.split('\\n\\n')\n",
    "    minibatch = [line for line in minibatch if not line.startswith(\":==:\")]\n",
    "stories, summaries = batch.process_minibatch(minibatch,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stories[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx=22\n",
    "' '.join(vocab.idx_list_to_word_list(stories[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch.idx2oov_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "' '.join(vocab.idx_list_to_word_list(stories[idx],batch.idx2oov_list[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unked = batch.unk_minibatch(stories[idx],vocab)\n",
    "' '.join(vocab.idx_list_to_word_list(unked,batch.idx2oov_list[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "' '.join(vocab.idx_list_to_word_list(summaries[idx],batch.idx2oov_list[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "help(nn.LSTMCell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch.oov2idx_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.ones([10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
